import torch
import pytorch_lightning as pl
from pathlib import Path
from models import models
from timm.scheduler.cosine_lr import CosineLRScheduler

class Tailor(pl.LightningModule):
    """
    The Tailor method of removing/shortening residual connections from a model.
    """
    def __init__(
        self, 
        teacher_model, 
        student_model=None, 
        learning_rate=0.2, 
        cosine_annealing=False,
        momentum=0.9, 
        weight_decay=1e-4,
        mse_loss_weight=0.35,
        student_loss_weight=0.65,
        kd=False,
        pretrained_model_ckpt=None,
        modifier=None,
        modifier_how_often=3,
        checkpoint_resume_path=None,
        **kwargs
    ):
        super().__init__()
        self.save_hyperparameters()
        self.teacher = models.get(teacher_model)()
        self.student = None if models.get(student_model, None) is None else models.get(student_model, None)()
        self.criterion = torch.nn.CrossEntropyLoss() # Teacher criterion
        self.student_criterion = torch.nn.CrossEntropyLoss()
        self.lr = learning_rate
        self.cosine_annealing = cosine_annealing
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.mse_loss_weight = mse_loss_weight
        self.student_loss_weight = student_loss_weight
        self.kd = student_model is not None and kd 
        # ---^^ Train using knowledge distillation
        self.modifier_how_often = modifier_how_often
        # Resuming from checkpoint
        self.resuming_from_ckpt = checkpoint_resume_path is not None
        if self.resuming_from_ckpt:
            print("Resuming from checkpoint")

        modifiers = {}
        if self.student:
            modifiers = {
                'remove': self.student.update_skip_removal, 
                'shorten': self.student.update_skip_shorten,
            }
        self.modifier = modifiers.get(modifier, None)
        
        # self.example_input_array = torch.Tensor(1, 3, 224, 224) # ImageNet dims
        self.example_input_array = torch.Tensor(1, 3, 32, 32) # cifar10 dims WOW

        if self.kd:
            if pretrained_model_ckpt is not None:
                self.load_ckpt_weights(pretrained_model_ckpt)
            # Disable teacher gradient updates, i.e., freeze the weights, for
            # knowledge distillation
            for param in self.teacher.parameters():
                param.requires_grad = False


    @staticmethod
    def add_model_specific_args(parent_parser):
        parser = parent_parser.add_argument_group('Tailor')
        parser.add_argument('--learning_rate', type=float, default=0.1)
        # Scale learning rate to number of gpus ~ lr(1gpu) * num_gpu
        parser.add_argument('--momentum', type=float, default=0.9)
        parser.add_argument('--weight_decay', type=float, default=1e-4)
        parser.add_argument(
            '--teacher_model', 
            type=str, 
            choices=models, 
            default='resnet50_imagenet'
        )
        parser.add_argument(
            '--student_model',
            type=str,
            choices=models,
            default=None
        )
        parser.add_argument('--kd', action='store_true', default=False)
        parser.add_argument('--pretrained_model_ckpt', type=Path, default=None)
        parser.add_argument('--modifier', type=str, default=None)
        parser.add_argument('--modifier_how_often', type=int, default=3)
        parser.add_argument('--checkpoint_resume_path', type=Path, default=None)
        parser.add_argument('--cosine_annealing', action='store_true', default=False)
        return parent_parser

    def load_ckpt_weights(self, model_ckpt):
        """
        Load teacher and student models with model_ckpt weights for KD
        
        NOTE: Assumes model_ckpt generated by this LightningModule
        """
        checkpoint = torch.load(model_ckpt)
        # Remove 'teacher' from checkpoint keys to match our model's keys
        sanitized_ckpt_state_dict = {}
        for key in checkpoint['state_dict'].keys():
            sanitized_key = key.replace('teacher.', '')
            sanitized_ckpt_state_dict[sanitized_key] = \
                checkpoint['state_dict'][key]
        self.teacher.load_state_dict(sanitized_ckpt_state_dict, strict=True)
        self.student.load_state_dict(sanitized_ckpt_state_dict, strict=False)
        # Student will have more/less params than teacher -------------^^^^^

    def accuracy(self, output, target, topk=(1,)):
        """Compute precision@k for the specified values of k."""
        with torch.no_grad():
            maxk = max(topk)
            batch_size = target.size(0)

            _, pred = output.topk(maxk, 1, True, True)
            pred = pred.t()

            # one-hot case
            if target.ndimension() > 1:
                target = target.max(1)[1]

            correct = pred.eq(target.view(1, -1).expand_as(pred))

            res = []
            for k in topk:
                correct_k = correct[:k].reshape(-1).float().sum(0)
                res.append(correct_k.mul_(1.0 / batch_size))

            return res

    def eval_step(self, batch, batch_idx, prefix):
        """
        For validation and testing evaluation
        """
        images, target = batch
        if self.kd: # For knowledge distillation, only eval student
            output = self.student(images)
            loss = self.student_criterion(output, target)
        else:
            output = self.teacher(images)
            # Compute loss
            loss = self.criterion(output, target)
        top1_acc, top5_acc = self.accuracy(output.detach(), target, topk=(1, 5))
        metrics = {
            f'{prefix}/Loss': loss,
            f'{prefix}/Top1': top1_acc,
            f'{prefix}/Top5': top5_acc
        }
        self.log_dict(
            metrics, 
            on_epoch=True, 
            logger=True, 
            sync_dist=False, 
            rank_zero_only=True
        )

    # Pytorch Lightning-specific methods
    def forward(self, x):
        return self.student(x) if self.kd else self.teacher(x) 

    def on_train_epoch_start(self):
        if self.resuming_from_ckpt \
        and self.modifier \
        and self.modifier(self.modifier_how_often, self.current_epoch - 1):
            # Update the student ResNet's shortcuts to match up to last
            # checkpoint if resuming 
            print('Modified for resuming from checkpoint!')
            self.resuming_from_ckpt = False


    def training_step(self, batch, batch_idx):
        images, target = batch
        if self.kd: # Knowledge distillation
            teacher_output = self.teacher(images)
            output = self.student(images)
            student_loss = self.student_criterion(output, target)
            # Compute KD loss
            weighted_mse_loss = self.mse_loss_weight \
                * torch.nn.MSELoss()(teacher_output, output)
            weighted_student_loss = self.student_loss_weight * student_loss 
            loss = weighted_mse_loss + weighted_student_loss
        else:
            output = self.teacher(images)
            # Compute loss
            loss = self.criterion(output, target)
        top1_acc, top5_acc = self.accuracy(output.detach(), target, topk=(1, 5))
        metrics = {
            'Train/Loss': loss,
            'Train/Top1': top1_acc,
            'Train/Top5': top5_acc,
        }
        self.log_dict(
            metrics, 
            on_step=True,
            on_epoch=True,
            sync_dist=False,
            rank_zero_only=True
        )
        return loss  

    def training_epoch_end(self, training_step_outputs):
        # For modifying the student ResNet's shortcuts during training
        if self.modifier and self.modifier(self.modifier_how_often, self.current_epoch):
            print('Modified!')

    def configure_optimizers(self):
        if self.kd:
            optimizer = torch.optim.SGD(
                params=self.student.parameters(), 
                lr=self.lr, 
                momentum=self.momentum, 
                weight_decay=self.weight_decay, 
                nesterov=True
            )
        else:
            optimizer = torch.optim.SGD(
                params=self.teacher.parameters(), 
                lr=self.lr, 
                momentum=self.momentum, 
                weight_decay=self.weight_decay, 
                nesterov=True
            )
        if self.cosine_annealing:
            lr_scheduler = CosineLRScheduler(
                optimizer,
                t_initial=120, # number of epochs for the whole schedule
                lr_min=1e-4,
                warmup_lr_init=self.lr, # initial learning rate
                warmup_t=10, # number of warmup epochs
                cycle_limit=1,
            )
        lr_scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=30, 
            gamma=0.1
        )
        return [optimizer], [lr_scheduler]

    def validation_step(self, batch, batch_idx):
        return self.eval_step(batch, batch_idx, "Val")
    
    def test_step(self, batch, batch_idx):
        return self.eval_step(batch, batch_idx, "Test")
    
    def predict_step(self, batch, batch_idx):
        images, target = batch
        return self.student(images) if self.kd else self.teacher(images)